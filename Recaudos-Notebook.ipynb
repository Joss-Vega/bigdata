{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook de Recaudos\n",
    "\n",
    "Esta notebook procesa datos de recaudos, realizando diversas transformaciones y uniones para consolidar la información necesaria para los reportes finales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importamos las librerías necesarias para el procesamiento de datos.\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.types import DecimalType\n",
    "from IPython.core.display import HTML\n",
    "from pyspark.sql.window import Window\n",
    "from datetime import datetime, timedelta\n",
    "from pyspark.sql.functions import coalesce, col, last\n",
    "\n",
    "# Ajuste para mostrar correctamente los datos en la notebook\n",
    "display(HTML(\"<style>pre { white-space: pre !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variables de Entorno\n",
    "\n",
    "Definimos las variables de entorno necesarias para el procesamiento de los datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cutoff_date = '2024-03-04' # 5 días posteriores al último mes\n",
    "fecha_procesar = '2024-02'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carga de Datos\n",
    "\n",
    "Cargamos los datos necesarios desde las rutas especificadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_recaudos = spark.read.parquet(\"/data/master/pmol/data/t_pmol_daily_clct_consolidated/cutoff_date=\" + fecha_procesar + '*')\n",
    "df_convenios = spark.read.parquet(\"/data/master/psan/data/t_psan_collection_agreements/cutoff_date=\" + cutoff_date)\n",
    "df_people = spark.read.parquet(\"/data/master/pbtq/data/t_pbtq_people_daily_information/cutoff_date=\" + cutoff_date)\n",
    "df_hierarchies = spark.read.parquet(\"/data/master/pcog/data/t_pcog_branch_hierarchies_daily/cutoff_date=\" + cutoff_date)\n",
    "df_tcom_seg = spark.read.parquet(\"/data/master/psag/data/t_psag_mthly_info_cust_seg_tcom/cutoff_date=2024-01-31\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selección de Columnas\n",
    "\n",
    "Seleccionamos las columnas relevantes de cada DataFrame para reducir el tamaño de los datos y enfocarnos solo en la información necesaria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seleccionamos las columnas de interés del DataFrame de personas.\n",
    "df_people = df_people.select('customer_id', 'personal_type', 'personal_id', 'last_name', 'second_last_name', 'customer_name', 'card_holder_name')\n",
    "\n",
    "# Seleccionamos las columnas de interés del DataFrame de convenios.\n",
    "df_convenios = df_convenios.select('entity_id', 'agreement_type', 'agreement_id', 'covenant_class_id', 'covenant_short_name', 'raise_money_cmsn_debit_type', 'current_contract_id', 'commission_account_id')\n",
    "\n",
    "# Filtramos y seleccionamos las columnas de interés del DataFrame de jerarquías.\n",
    "df_hierarchies = df_hierarchies.filter(df_hierarchies['entity_id'] == 9012).select('branch_id', 'branch_name', 'level50_territorial_desc', 'level60_operarea_desc')\n",
    "\n",
    "# Seleccionamos las columnas de interés del DataFrame de segmentación TCOM.\n",
    "df_tcom_seg = df_tcom_seg.select('customer_id', 'main_branchsf_id')\n",
    "\n",
    "# Renombramos la columna 'customer_id' a 'customer_tcom_id' para evitar conflictos en futuras uniones.\n",
    "df_tcom_seg = df_tcom_seg.withColumnRenamed('customer_id', 'customer_tcom_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uniones de DataFrames\n",
    "\n",
    "Realizamos las uniones necesarias entre los DataFrames para consolidar la información."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unimos los datos de segmentación TCOM con las jerarquías de sucursales.\n",
    "df_tcom_hierarchies = df_tcom_seg.join(df_hierarchies, df_hierarchies.branch_id == df_tcom_seg.main_branchsf_id, 'left')\n",
    "\n",
    "# Renombramos columnas en el DataFrame de recaudos para evitar conflictos en futuras uniones.\n",
    "df_recaudos = df_recaudos.withColumnRenamed('agreement_id', 'agreement_contrato_id')\n",
    "df_recaudos = df_recaudos.withColumnRenamed('covenant_class_id', 'covenant_class_contrato_id')\n",
    "\n",
    "# Unimos los recaudos con los convenios utilizando las columnas renombradas.\n",
    "df_join = df_recaudos.join(df_convenios, (df_convenios.agreement_id == df_recaudos.agreement_contrato_id) & (df_convenios.covenant_class_id == df_recaudos.covenant_class_contrato_id))\n",
    "\n",
    "# Unimos el resultado anterior con los datos de personas.\n",
    "df_joined = df_join.join(df_people, df_people.customer_id == df_join.ben_customer_id, 'left')\n",
    "\n",
    "# Finalmente, unimos el DataFrame resultante con los datos de jerarquías TCOM.\n",
    "df_joined = df_joined.join(df_tcom_hierarchies, df_tcom_hierarchies.customer_tcom_id == df_joined.ben_customer_id, 'left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformación de Datos\n",
    "\n",
    "Realizamos las transformaciones necesarias en los datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertimos las columnas de importes a tipo decimal para asegurar precisión en los cálculos.\n",
    "df_joined = df_joined.withColumn('monto_soles', (F.col('local_currency_amount')).cast('decimal(23,2)'))\n",
    "df_joined = df_joined.withColumn('comision_empresa_soles', (F.col('local_currency_company_fee_amount')).cast('decimal(23,2)'))\n",
    "df_joined = df_joined.withColumn('comision_cliente_soles', (F.col('local_currency_customer_fee_amount')).cast('decimal(23,2)'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agregación de Datos\n",
    "\n",
    "Agrupamos y agregamos los datos para obtener las métricas finales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agrupamos los datos por diversas dimensiones y calculamos métricas agregadas como sumas de montos y comisiones, y el conteo de operaciones.\n",
    "df_final = df_joined.groupBy('agreement_id', 'covenant_class_id', 'ben_customer_id', 'card_holder_name', 'covenant_short_name', 'level50_territorial_desc', 'level60_operarea_desc', 'branch_name', 'payment_channel_type')\\\n",
    "                    .agg(F.sum('monto_soles').cast(\"decimal(23,2)\").alias('monto_pagado'),\n",
    "                         F.sum('comision_empresa_soles').cast(\"decimal(23,2)\").alias('comision_empresa'),\n",
    "                         F.sum('comision_cliente_soles').cast(\"decimal(23,2)\").alias('comision_cliente'),\n",
    "                         F.count('*').alias('numero_operaciones'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Almacenamiento de Datos\n",
    "\n",
    "Guardamos los datos finales en el almacenamiento especificado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardamos el DataFrame final en formato Parquet, coalesciendo a un único archivo.\n",
    "df_final.coalesce(1).write.parquet(path=\"/intelligence/inpdpm/analytic/users/PRUEBA/workspace/OfRe/reportes/\" + fecha_procesar, mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resumen de Transformaciones y Funciones Usadas\n",
    "\n",
    "- **select**: Selecciona columnas específicas de un DataFrame, lo que ayuda a reducir el tamaño de los datos y enfocarse solo en la información necesaria.\n",
    "- **withColumnRenamed**: Renombra una columna existente en el DataFrame, útil para evitar conflictos de nombres en futuras uniones.\n",
    "- **join**: Realiza una unión entre dos DataFrames basándose en una condición específica, permitiendo combinar información de múltiples fuentes.\n",
    "- **withColumn**: Crea o reemplaza una columna en el DataFrame. En este caso, se usa para convertir importes a tipo decimal.\n",
    "- **groupBy**: Agrupa los datos por una o más columnas, permitiendo realizar operaciones de agregación en cada grupo.\n",
    "- **agg**: Realiza operaciones de agregación, como sumas y conteos, en los datos agrupados.\n",
    "- **coalesce**: Reduce el número de particiones en el DataFrame, útil para escribir datos en un solo archivo.\n",
    "- **write.parquet**: Guarda el DataFrame en formato Parquet, que es eficiente en términos de almacenamiento y rendimiento para grandes volúmenes de datos."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
